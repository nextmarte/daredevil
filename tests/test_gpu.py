#!/usr/bin/env python
"""
Script para testar a configura√ß√£o de GPU no ambiente Daredevil
"""
import sys

def test_torch_gpu():
    """Testa se PyTorch detecta GPU"""
    print("=" * 60)
    print("TESTE DE GPU - PyTorch")
    print("=" * 60)
    
    try:
        import torch
        print(f"‚úì PyTorch instalado: {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"‚úì CUDA dispon√≠vel: {torch.version.cuda}")
            print(f"‚úì N√∫mero de GPUs: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                print(f"\nGPU {i}:")
                print(f"  - Nome: {torch.cuda.get_device_name(i)}")
                props = torch.cuda.get_device_properties(i)
                print(f"  - Mem√≥ria Total: {props.total_memory / (1024**3):.2f} GB")
                print(f"  - Compute Capability: {props.major}.{props.minor}")
                print(f"  - Multiprocessors: {props.multi_processor_count}")
        else:
            print("‚úó CUDA n√£o dispon√≠vel")
            print("Poss√≠veis causas:")
            print("  - GPU NVIDIA n√£o detectada")
            print("  - Drivers NVIDIA n√£o instalados")
            print("  - PyTorch instalado sem suporte CUDA")
            return False
            
        return True
        
    except ImportError:
        print("‚úó PyTorch n√£o instalado")
        print("Instale com: uv add torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        return False


def test_whisper_gpu():
    """Testa se Whisper pode usar GPU"""
    print("\n" + "=" * 60)
    print("TESTE DE GPU - Whisper")
    print("=" * 60)
    
    try:
        import whisper
        import torch
        
        print(f"‚úì Whisper instalado")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"‚úì Dispositivo selecionado: {device}")
        
        if device == "cuda":
            print("\nCarregando modelo Whisper tiny na GPU (teste r√°pido)...")
            model = whisper.load_model("tiny", device=device)
            print(f"‚úì Modelo carregado com sucesso na GPU")
            
            # Verificar mem√≥ria alocada
            memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            print(f"‚úì Mem√≥ria GPU alocada: {memory_allocated:.2f} GB")
            
            return True
        else:
            print("‚úó GPU n√£o dispon√≠vel para Whisper")
            return False
            
    except ImportError:
        print("‚úó Whisper n√£o instalado")
        print("Instale com: uv add openai-whisper")
        return False
    except Exception as e:
        print(f"‚úó Erro ao carregar modelo Whisper: {e}")
        return False


def test_nvidia_smi():
    """Testa se nvidia-smi est√° dispon√≠vel"""
    print("\n" + "=" * 60)
    print("TESTE DE GPU - nvidia-smi")
    print("=" * 60)
    
    import subprocess
    
    try:
        result = subprocess.run(
            ['nvidia-smi'],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            print("‚úì nvidia-smi dispon√≠vel")
            print("\nSa√≠da do nvidia-smi:")
            print("-" * 60)
            print(result.stdout)
            return True
        else:
            print("‚úó nvidia-smi retornou erro")
            print(result.stderr)
            return False
            
    except FileNotFoundError:
        print("‚úó nvidia-smi n√£o encontrado")
        print("Certifique-se de que os drivers NVIDIA est√£o instalados")
        return False
    except Exception as e:
        print(f"‚úó Erro ao executar nvidia-smi: {e}")
        return False


def test_docker_gpu():
    """Verifica se est√° rodando em Docker com GPU"""
    print("\n" + "=" * 60)
    print("TESTE DE AMBIENTE")
    print("=" * 60)
    
    import os
    
    # Verificar se est√° em container
    if os.path.exists('/.dockerenv'):
        print("‚úì Rodando em container Docker")
        
        # Verificar vari√°veis de ambiente relacionadas a GPU
        cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES')
        if cuda_visible:
            print(f"‚úì CUDA_VISIBLE_DEVICES: {cuda_visible}")
        else:
            print("  CUDA_VISIBLE_DEVICES n√£o definida")
            
    else:
        print("‚úì Rodando no host (n√£o em container)")
    
    # Verificar diret√≥rios CUDA
    if os.path.exists('/usr/local/cuda'):
        print("‚úì Diret√≥rio CUDA encontrado: /usr/local/cuda")
    else:
        print("  Diret√≥rio CUDA n√£o encontrado")
    
    return True


def main():
    """Executa todos os testes"""
    print("\nüöÄ Daredevil GPU Test Suite\n")
    
    results = {
        "nvidia-smi": test_nvidia_smi(),
        "torch": test_torch_gpu(),
        "whisper": test_whisper_gpu(),
        "environment": test_docker_gpu(),
    }
    
    print("\n" + "=" * 60)
    print("RESUMO DOS TESTES")
    print("=" * 60)
    
    all_passed = True
    for test_name, passed in results.items():
        status = "‚úì PASSOU" if passed else "‚úó FALHOU"
        print(f"{test_name:20s}: {status}")
        if not passed:
            all_passed = False
    
    print("=" * 60)
    
    if all_passed:
        print("\nüéâ Todos os testes passaram! GPU configurada corretamente.")
        return 0
    else:
        print("\n‚ö†Ô∏è  Alguns testes falharam. Verifique os detalhes acima.")
        print("\nPara mais informa√ß√µes, consulte GPU_SETUP.md")
        return 1


if __name__ == "__main__":
    sys.exit(main())
