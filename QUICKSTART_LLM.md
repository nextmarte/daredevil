# Guia R√°pido - Integra√ß√£o LLM Qwen3:30b

## üöÄ Setup em 3 Passos

### 1. Instalar Ollama
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Baixar Modelo
```bash
ollama pull qwen3:30b
```
‚è±Ô∏è Aguarde o download (~17GB, pode levar alguns minutos)

### 3. Iniciar Servidor
```bash
ollama serve
```
‚úÖ Servidor rodando em `http://localhost:11434`

## üéØ Uso Imediato

### Via API (cURL)
```bash
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@seu_audio.mp3" \
  -F "use_llm=true"
```

### Via Python
```python
from transcription.services import TranscriptionService

result = TranscriptionService.process_audio_file(
    file_path="audio.mp3",
    use_llm=True
)
print(result.transcription.text)
```

### Configura√ß√£o Permanente
```bash
# No arquivo .env
USE_LLM_POST_PROCESSING=true
LLM_MODEL=qwen3:30b
# OLLAMA_HOST=http://localhost:11434  # Opcional, padr√£o j√° √© localhost:11434
```

## üß™ Testar Instala√ß√£o

```bash
# Verificar se Ollama est√° rodando
curl http://localhost:11434/api/tags

# Executar demo
uv run python demo_llm_post_processing.py
```

## ‚ö° Dicas de Performance

### Hardware Recomendado
- **M√≠nimo:** 16GB RAM, CPU multi-core
- **Ideal:** 32GB RAM, GPU NVIDIA (CUDA)

### Usar Modelo Menor (Mais R√°pido)
```bash
# Instalar modelo menor
ollama pull qwen2.5:7b

# Configurar
LLM_MODEL=qwen2.5:7b
```

## üîß Solu√ß√£o R√°pida de Problemas

### "Connection error"
```bash
# Verificar se Ollama est√° rodando
ps aux | grep ollama

# Se n√£o estiver, iniciar
ollama serve
```

### "Model not found"
```bash
# Baixar o modelo
ollama pull qwen3:30b

# Listar modelos instalados
ollama list
```

### "Out of memory"
```bash
# Usar modelo menor
ollama pull qwen2.5:3b
LLM_MODEL=qwen2.5:3b
```

## üìä Compara√ß√£o R√°pida

| Caracter√≠stica | Tradicional | LLM |
|---------------|-------------|-----|
| Velocidade | ‚ö°‚ö°‚ö° | ‚ö°‚ö° |
| Qualidade | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Requisitos | Baixos | M√©dios/Altos |
| Offline | ‚úÖ | ‚úÖ |

## üìö Documenta√ß√£o Completa

- **Detalhes:** [LLM_INTEGRATION.md](LLM_INTEGRATION.md)
- **API:** [README.md](README.md)
- **Testes:** `transcription/test_llm_post_processing.py`

## üí° Exemplos de Uso

### Transcri√ß√£o Simples
```bash
curl -X POST http://localhost:8000/api/transcribe \
  -F "file=@audio.mp3" \
  -F "use_llm=true"
```

### Com Todas as Op√ß√µes
```bash
curl -X POST http://localhost:8000/api/transcribe \
  -F "file=@audio.mp3" \
  -F "language=pt" \
  -F "use_llm=true" \
  -F "identify_speakers=true" \
  -F "correct_grammar=true" \
  -F "clean_hesitations=true"
```

### Desabilitar LLM para um Request
```bash
curl -X POST http://localhost:8000/api/transcribe \
  -F "file=@audio.mp3" \
  -F "use_llm=false"
```

## ‚úÖ Checklist de Instala√ß√£o

- [ ] Ollama instalado (`ollama --version`)
- [ ] Modelo baixado (`ollama list | grep qwen3`)
- [ ] Servidor rodando (`curl http://localhost:11434/api/tags`)
- [ ] Vari√°veis de ambiente configuradas (`.env`)
- [ ] API funcionando (`curl http://localhost:8000/api/health`)
- [ ] Demo executado com sucesso (`uv run python demo_llm_post_processing.py`)

## üéâ Pronto!

Agora voc√™ pode usar o poder do LLM Qwen3:30b para melhorar suas transcri√ß√µes!

Para mais informa√ß√µes, consulte a [documenta√ß√£o completa](LLM_INTEGRATION.md).
