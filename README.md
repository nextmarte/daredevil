# Daredevil - API de Transcri√ß√£o de √Åudio e V√≠deo üéôÔ∏è

API completa de transcri√ß√£o de √°udio e v√≠deo em portugu√™s brasileiro usando Django Ninja e Whisper (OpenAI). Suporta m√∫ltiplos formatos, acelera√ß√£o por GPU NVIDIA, e processamento inteligente de texto.

## ‚ö° Performance Otimizada (NOVO!)

**Melhorias implementadas para velocidade 2-3x maior:**

- üöÄ **Cache Inteligente**: Resultados salvos automaticamente (LRU + TTL)
- üéØ **GPU Persistente**: Modelo mantido em mem√≥ria GPU entre requisi√ß√µes
- üîÑ **Processamento Ass√≠ncrono**: Celery + Redis para jobs em background
- üìä **Monitoramento Real-time**: Status de GPU e cache via API
- üîÅ **Retry Autom√°tico**: Jobs falhos s√£o retentados automaticamente
- üì¢ **Webhooks**: Notifica√ß√£o autom√°tica quando transcri√ß√£o completa

**Performance esperada:**
- Audio 1min: ~12s ‚Üí **<8s** (cache: **<0.1s**)
- Video 5min: ~60s ‚Üí **<45s** (cache: **<0.1s**)
- Video 30min: ~5min ‚Üí **<2min**

üìñ **[Ver documenta√ß√£o completa de otimiza√ß√µes ‚Üí](PERFORMANCE_OPTIMIZATION.md)**

## üöÄ Caracter√≠sticas Principais

- ‚úÖ **Transcri√ß√£o de alta qualidade** usando Whisper (OpenAI)
- ‚úÖ **Otimizado para portugu√™s brasileiro** com p√≥s-processamento autom√°tico
- ‚úÖ **Acelera√ß√£o por GPU NVIDIA (CUDA 12.1)** - processamento at√© 10x mais r√°pido
- ‚úÖ **Cache inteligente** - resultados instant√¢neos para arquivos j√° processados
- ‚úÖ **Processamento ass√≠ncrono** - ideal para arquivos grandes e lotes
- ‚úÖ **Suporte a v√≠deos** - extra√ß√£o autom√°tica de √°udio de 12 formatos de v√≠deo
- ‚úÖ **Suporte a m√∫ltiplos formatos de √°udio** - WhatsApp, Instagram e formatos padr√£o
- ‚úÖ **Transcri√ß√£o com timestamps detalhados** - precis√£o ao n√≠vel de segmento
- ‚úÖ **Processamento em lote** - m√∫ltiplos arquivos simultaneamente
- ‚úÖ **API RESTful moderna** com Django Ninja
- ‚úÖ **Documenta√ß√£o autom√°tica** (Swagger/OpenAPI)
- ‚úÖ **Valida√ß√£o autom√°tica** com Pydantic
- ‚úÖ **Deploy com Docker** - pronto para produ√ß√£o
- ‚úÖ **Limite de 500MB** por arquivo

## üìã √çndice

- [Requisitos](#-requisitos)
- [Instala√ß√£o](#Ô∏è-instala√ß√£o)
- [Docker](#-docker)
- [Uso da API](#-uso-da-api)
- [Formatos Suportados](#-formatos-suportados)
- [GPU NVIDIA](#-gpu-nvidia)
- [Portugu√™s Brasileiro](#-portugu√™s-brasileiro)
- [Processamento de V√≠deos](#-processamento-de-v√≠deos)
- [Configura√ß√£o](#Ô∏è-configura√ß√£o)
- [Testes](#-testes)
- [Performance](#-performance)
- [Troubleshooting](#-troubleshooting)
- [Estrutura do Projeto](#Ô∏è-estrutura-do-projeto)

## üìã Requisitos

### Software Necess√°rio

- **Python 3.12+**
- **uv** (gerenciador de pacotes Python)
- **ffmpeg** (para processamento de √°udio/v√≠deo)

### Requisitos de Hardware

#### M√≠nimo (modelo `tiny` ou `base`)
- RAM: 2GB dispon√≠vel
- Disco: 500MB para modelo + 1GB para cache
- CPU: Qualquer processador moderno

#### Recomendado (modelo `medium`)
- RAM: 6GB dispon√≠vel
- Disco: 1GB para modelo + 2GB para cache
- CPU: 4+ cores
- **GPU NVIDIA (opcional)**: Acelera significativamente o processamento

#### Performance (modelo `large`)
- RAM: 12GB dispon√≠vel
- Disco: 2GB para modelo + 3GB para cache
- CPU: 8+ cores ou GPU NVIDIA com CUDA
- **GPU NVIDIA**: Altamente recomendado

### Instalar Depend√™ncias do Sistema

```bash
# Ubuntu/Debian
sudo apt-get update && sudo apt-get install -y ffmpeg

# macOS
brew install ffmpeg

# Arch Linux
sudo pacman -S ffmpeg

# Verificar instala√ß√£o
ffmpeg -version
```

### Instalar uv (Gerenciador de Pacotes Python)

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

## üõ†Ô∏è Instala√ß√£o

### Op√ß√£o 1: Instala√ß√£o Local

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/nextmarte/daredevil.git
cd daredevil

# 2. Instale as depend√™ncias com uv
uv sync

# 3. Configure as vari√°veis de ambiente
cp .env.example .env
# Edite o .env conforme necess√°rio

# 4. Execute as migra√ß√µes
uv run python manage.py migrate

# 5. Inicie o servidor
uv run python manage.py runserver
```

A API estar√° dispon√≠vel em: `http://localhost:8000/api/`

### Op√ß√£o 2: Docker (Recomendado)

Veja a se√ß√£o [Docker](#-docker) abaixo.

## üê≥ Docker

### Quick Start

```bash
# Build e iniciar container
docker compose up -d

# Ver logs
docker compose logs -f web

# Parar container
docker compose down
```

O servidor estar√° dispon√≠vel em: `http://localhost:8511/api/`

### Com GPU NVIDIA

Para usar GPU, voc√™ precisa ter o NVIDIA Container Toolkit instalado:

```bash
# Ubuntu/Debian - Instalar NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verificar instala√ß√£o
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

Ent√£o, basta iniciar o container normalmente:

```bash
docker compose up -d
```

### Comandos √öteis do Docker

```bash
# Parar container
docker compose down

# Rebuild completo (for√ßa reconstru√ß√£o)
docker compose build --no-cache web

# Restart
docker compose restart web

# Ver status
docker compose ps

# Entrar no container
docker exec -it daredevil_web /bin/bash

# Ver logs
docker compose logs -f web

# Executar comando no container
docker exec daredevil_web uv run python manage.py <comando>
```

## üéØ Uso da API

### Documenta√ß√£o Autom√°tica

Ap√≥s iniciar o servidor, acesse:

- **Swagger UI**: `http://localhost:8000/api/docs` (ou `http://localhost:8511/api/docs` no Docker)
- **ReDoc**: `http://localhost:8000/api/redoc`
- **OpenAPI Schema**: `http://localhost:8000/api/openapi.json`

### Endpoints Principais

#### Health Check

```bash
GET /api/health
```

Verifica o status da API e configura√ß√µes.

**Exemplo:**
```bash
curl http://localhost:8000/api/health
```

#### GPU Status

```bash
GET /api/gpu-status
```

Verifica se GPU est√° dispon√≠vel e mostra informa√ß√µes de mem√≥ria.

**Exemplo de resposta com GPU:**
```json
{
  "gpu_available": true,
  "device": "cuda",
  "gpu_count": 2,
  "gpus": [
    {
      "id": 0,
      "name": "NVIDIA GeForce RTX 3060",
      "memory_allocated_gb": 2.5,
      "memory_reserved_gb": 3.0,
      "memory_total_gb": 12.0,
      "memory_free_gb": 9.0,
      "compute_capability": "8.6"
    }
  ]
}
```

#### Listar Formatos Suportados

```bash
GET /api/formats
```

Lista todos os formatos de √°udio e v√≠deo suportados.

**Resposta:**
```json
{
  "audio_formats": ["aac", "m4a", "mp3", "ogg", "opus", "wav", "flac", "webm", "weba"],
  "video_formats": ["mp4", "avi", "mov", "mkv", "flv", "wmv", "webm", "ogv", "ts", "mts", "m2ts", "3gp", "f4v", "asf"],
  "all_formats": ["aac", "m4a", ...],
  "max_file_size_mb": 500
}
```

#### Transcrever √Åudio ou V√≠deo

```bash
POST /api/transcribe
```

**Par√¢metros:**
- `file`: Arquivo de √°udio ou v√≠deo (multipart/form-data)
- `language`: C√≥digo do idioma (padr√£o: "pt" - portugu√™s brasileiro)
- `model`: Modelo Whisper (opcional: tiny, base, small, medium, large)

**Exemplo com curl - √Åudio:**
```bash
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=pt"
```

**Exemplo com curl - V√≠deo:**
```bash
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@video.mp4" \
  -F "language=pt"
```

**Exemplo com Python:**
```python
import requests

url = "http://localhost:8000/api/transcribe"
files = {"file": open("audio.mp3", "rb")}
data = {"language": "pt"}

response = requests.post(url, files=files, data=data)
print(response.json())
```

**Resposta:**
```json
{
  "success": true,
  "transcription": {
    "text": "Ol√°, como voc√™ est√°?",
    "segments": [
      {
        "start": 0.0,
        "end": 2.5,
        "text": "Ol√°, como voc√™ est√°?",
        "confidence": 0.95
      }
    ],
    "language": "pt",
    "duration": 2.5
  },
  "processing_time": 3.2,
  "audio_info": {
    "format": "mp3",
    "duration": 2.5,
    "sample_rate": 44100,
    "channels": 2,
    "file_size_mb": 0.5
  }
}
```

#### Transcrever em Lote

```bash
POST /api/transcribe/batch
```

**Par√¢metros:**
- `files`: Lista de arquivos de √°udio/v√≠deo
- `language`: C√≥digo do idioma (padr√£o: "pt")
- `model`: Modelo Whisper (opcional)

**Exemplo:**
```bash
curl -X POST "http://localhost:8000/api/transcribe/batch" \
  -F "files=@audio1.mp3" \
  -F "files=@video2.mp4" \
  -F "files=@audio3.wav" \
  -F "language=pt"
```

#### Transcrever Ass√≠ncrono (NOVO! ‚ö°)

```bash
POST /api/transcribe/async
```

Para arquivos grandes ou quando n√£o quer bloquear a requisi√ß√£o.

**Par√¢metros:**
- `file`: Arquivo de √°udio ou v√≠deo
- `language`: C√≥digo do idioma (padr√£o: "pt")
- `model`: Modelo Whisper (opcional)
- `webhook_url`: URL para notifica√ß√£o quando concluir (opcional)

**Exemplo:**
```bash
curl -X POST "http://localhost:8000/api/transcribe/async" \
  -F "file=@video_longo.mp4" \
  -F "language=pt" \
  -F "webhook_url=https://meusite.com/webhook"
```

**Resposta:**
```json
{
  "success": true,
  "task_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "status_url": "/api/transcribe/async/status/a1b2c3d4...",
  "message": "Transcri√ß√£o iniciada. Use task_id para consultar o status.",
  "submission_time": 0.15
}
```

#### Consultar Status de Tarefa Ass√≠ncrona (NOVO! ‚ö°)

```bash
GET /api/transcribe/async/status/{task_id}
```

**Exemplo:**
```bash
curl http://localhost:8000/api/transcribe/async/status/a1b2c3d4-e5f6-7890-abcd-ef1234567890
```

**Resposta (conclu√≠da):**
```json
{
  "task_id": "a1b2c3d4...",
  "state": "SUCCESS",
  "result": {
    "success": true,
    "transcription": {
      "text": "transcri√ß√£o completa...",
      "segments": [...],
      "language": "pt"
    },
    "processing_time": 45.2
  },
  "message": "Transcri√ß√£o conclu√≠da"
}
```

#### Cancelar Tarefa Ass√≠ncrona (NOVO! ‚ö°)

```bash
DELETE /api/transcribe/async/{task_id}
```

#### Estat√≠sticas de Cache (NOVO! ‚ö°)

```bash
GET /api/cache-stats
```

Retorna estat√≠sticas do cache (hits, misses, hit rate).

**Resposta:**
```json
{
  "cache_enabled": true,
  "size": 25,
  "max_size": 100,
  "hits": 150,
  "misses": 50,
  "hit_rate": 75.0,
  "ttl_seconds": 3600
}
```

#### Limpar Cache (NOVO! ‚ö°)

```bash
POST /api/cache/clear
```

Remove todos os itens do cache.

## üìÅ Formatos Suportados

### √Åudio (9 formatos)

- `.aac` - Advanced Audio Coding
- `.m4a` - MPEG-4 Audio
- `.mp3` - MPEG Audio Layer 3
- `.ogg` - Ogg Vorbis
- `.opus` - Opus Audio (WhatsApp)
- `.wav` - Waveform Audio
- `.flac` - Free Lossless Audio Codec
- `.webm` - WebM Audio
- `.weba` - WebM Audio

### V√≠deo (14 formatos)

O sistema extrai automaticamente o √°udio de arquivos de v√≠deo:

- `.mp4` - MPEG-4 Video (WhatsApp, Instagram)
- `.avi` - Audio Video Interleave
- `.mov` - QuickTime (iPhone, macOS)
- `.mkv` - Matroska
- `.flv` - Flash Video
- `.wmv` - Windows Media Video
- `.webm` - WebM Video
- `.ogv` - Ogg Video
- `.ts` - MPEG Transport Stream
- `.mts` - MPEG Transport Stream (Sony)
- `.m2ts` - MPEG-2 Transport Stream (Blu-ray)
- `.3gp` - 3GPP (celulares)
- `.f4v` - Flash Video
- `.asf` - Advanced Systems Format

**Total: 23 formatos suportados**

**Limite de tamanho:** 500MB por arquivo (configur√°vel)

## üéÆ GPU NVIDIA

### Benef√≠cios da GPU

- ‚ö° Processamento **5-10x mais r√°pido**
- üöÄ Suporte a modelos maiores (large) sem lentid√£o
- üì¶ Melhor para processamento em lote
- üíæ Uso eficiente de mem√≥ria com FP16

### Configura√ß√£o

O projeto j√° est√° configurado para usar GPU automaticamente quando dispon√≠vel. O Dockerfile usa a imagem base `nvidia/cuda:12.1.0-base-ubuntu22.04` e o `docker-compose.yml` j√° tem a configura√ß√£o de GPU:

```yaml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all  # Usa todas as GPUs dispon√≠veis
          capabilities: [gpu]
```

### Verificar GPU

```bash
# No host
nvidia-smi

# No container Docker
docker exec daredevil_web nvidia-smi

# Via API
curl http://localhost:8511/api/gpu-status
```

### Modelos Whisper e Requisitos de GPU

| Modelo | Tamanho | RAM Necess√°ria | GPU VRAM | Velocidade (CPU) | Velocidade (GPU) | Qualidade |
|--------|---------|----------------|----------|------------------|------------------|-----------|
| tiny   | ~39 MB  | ~1 GB          | ~1 GB    | Muito r√°pido     | Extremamente r√°pido | B√°sica |
| base   | ~74 MB  | ~1 GB          | ~1 GB    | R√°pido           | Muito r√°pido     | Boa    |
| small  | ~244 MB | ~2 GB          | ~2 GB    | Moderado         | R√°pido           | Muito boa |
| medium | ~769 MB | ~5 GB          | ~5 GB    | Lento            | Moderado         | Excelente |
| large  | ~1.5 GB | ~10 GB         | ~10 GB   | Muito lento      | Moderado         | Melhor |

**Recomenda√ß√£o:** 
- Com CPU: Use `small` ou `medium`
- Com GPU: Use `medium` ou `large` para melhor qualidade

### Performance Esperada com GPU

Com GPU habilitada (RTX 3060 ou superior):

- **Whisper base**: ~5-10x mais r√°pido
- **Whisper small**: ~4-8x mais r√°pido
- **Whisper medium**: ~3-6x mais r√°pido
- **Whisper large**: ~2-4x mais r√°pido

**Exemplo pr√°tico:**
- √Åudio de 5 minutos com modelo medium:
  - **CPU**: ~45-60 segundos
  - **GPU**: ~8-15 segundos

## üáßüá∑ Portugu√™s Brasileiro

### Funcionalidades

A API foi totalmente otimizada para portugu√™s brasileiro:

- ‚úÖ **Portugu√™s como idioma padr√£o** - n√£o precisa especificar language=pt
- ‚úÖ **P√≥s-processamento inteligente** de texto
- ‚úÖ **Remo√ß√£o de hesita√ß√µes** comuns (tipo, sabe, entendeu, n√©, h√£, etc.)
- ‚úÖ **Normaliza√ß√£o de pontua√ß√£o** e capitaliza√ß√£o
- ‚úÖ **Expans√£o de abrevia√ß√µes** (Sr., Ltda., etc.)
- ‚úÖ **Corre√ß√£o de erros comuns** do Whisper em portugu√™s

### Exemplo de Processamento

**Entrada (bruta do Whisper):**
```
Ent√£o tipo voc√™ sabe n√© isso √© bem importante h√£ . O sr jo√£o trabalha na ltda .
```

**Sa√≠da (processada):**
```
Ent√£o, voc√™ sabe, isso √© bem importante. O Sr. Jo√£o trabalha na Ltda.
```

### Melhorias Aplicadas

#### 1. Remo√ß√£o de Hesita√ß√µes
Remove palavras de hesita√ß√£o comuns: tipo, sabe, entendeu, n√©, t√°, h√£, hm, hmm, ah, √©, etc.

#### 2. Normaliza√ß√£o de Pontua√ß√£o
- Remove espa√ßos antes de pontua√ß√£o
- Adiciona espa√ßo ap√≥s pontua√ß√£o
- Corrige m√∫ltiplas pontua√ß√µes

#### 3. Capitaliza√ß√£o Correta
- Primeira letra do texto mai√∫scula
- Primeira letra ap√≥s pontua√ß√£o final
- Nomes pr√≥prios reconhecidos

#### 4. Expans√£o de Abrevia√ß√µes
- sr ‚Üí Sr.
- sra ‚Üí Sra.
- dr ‚Üí Dr.
- ltda ‚Üí Ltda.
- etc ‚Üí Etc.

### Usar Outro Idioma

Voc√™ ainda pode transcrever em outros idiomas:

```bash
# Ingl√™s
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=en"

# Espanhol
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=es"
```

## üé• Processamento de V√≠deos

### Como Funciona

Quando um arquivo de v√≠deo √© enviado, o sistema automaticamente:

1. ‚úÖ Valida a integridade do arquivo com `ffprobe`
2. ‚úÖ Extrai o √°udio em qualidade otimizada (16kHz, mono, WAV)
3. ‚úÖ Transcreve o √°udio com Whisper
4. ‚úÖ Aplica processamento de portugu√™s brasileiro
5. ‚úÖ Retorna transcri√ß√£o com timestamps
6. ‚úÖ Limpa arquivos tempor√°rios

### Exemplo de Uso

```bash
# Transcrever v√≠deo do Instagram
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@instagram_video.mp4" \
  -F "language=pt"

# Transcrever v√≠deo do iPhone
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@iphone_video.mov" \
  -F "model=large"
```

### Performance com V√≠deos

Tempos t√≠picos com GPU RTX 3060:

| Dura√ß√£o do V√≠deo | Tempo de Processamento |
|------------------|------------------------|
| 1 minuto         | ~15-20 segundos        |
| 5 minutos        | ~30-40 segundos        |
| 30 minutos       | ~2-3 minutos           |
| 1 hora           | ~4-6 minutos           |

**Fatores que afetam:** Tamanho/resolu√ß√£o do v√≠deo, bitrate de √°udio, modelo Whisper usado, disponibilidade de GPU.

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# Modelo Whisper (tiny, base, small, medium, large)
WHISPER_MODEL=medium

# Idioma padr√£o
WHISPER_LANGUAGE=pt

# Tamanho m√°ximo de arquivo em MB
MAX_AUDIO_SIZE_MB=500

# Diret√≥rio tempor√°rio
TEMP_AUDIO_DIR=/tmp/daredevil

# Habilitar cache
ENABLE_CACHE=true

# N√≠vel de log
LOG_LEVEL=INFO

# Django
DEBUG=0
ALLOWED_HOSTS=*
SECRET_KEY=your-secret-key-here

# Locale (Portugu√™s Brasileiro)
LANGUAGE=pt_BR.UTF-8
LANG=pt_BR.UTF-8
LC_ALL=pt_BR.UTF-8
```

### Docker Compose

Edite `docker-compose.yml` para ajustar configura√ß√µes:

```yaml
environment:
  - WHISPER_MODEL=medium
  - WHISPER_LANGUAGE=pt
  - MAX_AUDIO_SIZE_MB=500
  - LANGUAGE=pt_BR.UTF-8
  - LANG=pt_BR.UTF-8
  - LC_ALL=pt_BR.UTF-8
```

### Ajustar Limite de Upload

Para aumentar o limite al√©m de 500MB:

```env
# No .env
MAX_AUDIO_SIZE_MB=1000  # 1GB
```

Depois reinicie o servidor ou container.

## üß™ Testes

### Testes Dispon√≠veis

```bash
# Testar configura√ß√£o da GPU
uv run python test_gpu.py

# Testar portugu√™s brasileiro
uv run python test_portuguese_br.py

# Testar suporte a v√≠deos
uv run python test_video_support.py

# Testar processamento de portugu√™s
uv run python test_pt_processing.py

# Testar API completa
uv run python test_api.py
```

### Teste R√°pido via curl

```bash
# Health check
curl http://localhost:8000/api/health

# Status da GPU
curl http://localhost:8000/api/gpu-status

# Listar formatos
curl http://localhost:8000/api/formats

# Transcrever √°udio de teste
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@seu_audio.mp3" \
  -F "language=pt"
```

### Teste com Python

```python
import requests

def test_transcription():
    url = "http://localhost:8000/api/transcribe"
    files = {"file": open("audio.mp3", "rb")}
    data = {"language": "pt"}
    
    response = requests.post(url, files=files, data=data)
    result = response.json()
    
    if result["success"]:
        print(f"‚úÖ Transcri√ß√£o: {result['transcription']['text']}")
        print(f"‚è±Ô∏è Tempo: {result['processing_time']:.2f}s")
    else:
        print(f"‚ùå Erro: {result.get('error')}")

test_transcription()
```

## üìä Performance

### Tempos de Transcri√ß√£o (1 minuto de √°udio)

| Modelo | CPU (8 cores) | GPU (RTX 3060) | Speedup |
|--------|---------------|----------------|---------|
| tiny   | ~30s          | ~3-5s          | 6-10x   |
| base   | ~45s          | ~5-8s          | 6-9x    |
| small  | ~60s          | ~8-12s         | 6-8x    |
| medium | ~90s          | ~12-18s        | 6-7x    |
| large  | ~120s         | ~18-25s        | 5-7x    |

### Benchmarks Reais

Com GPU RTX 3060 (12GB):

- **√Åudio WhatsApp (30s, opus)**: ~5-8s
- **V√≠deo Instagram (1min, mp4)**: ~15-20s
- **Podcast (30min, mp3)**: ~2-3min
- **Entrevista (1h, wav)**: ~4-6min

### Otimiza√ß√µes Ativas

- ‚úÖ GPU NVIDIA com CUDA 12.1
- ‚úÖ FP16 em GPU (economiza 50% de mem√≥ria)
- ‚úÖ Portugu√™s como padr√£o (sem overhead de detec√ß√£o)
- ‚úÖ P√≥s-processamento otimizado (~0.1-0.2s overhead)
- ‚úÖ Cache de modelos em mem√≥ria GPU
- ‚úÖ Processamento paralelo de √°udio (ffmpeg multi-thread)

## üêõ Troubleshooting

### Problemas Comuns

#### 1. "Connection refused"

**Problema:** Servidor n√£o est√° rodando.

**Solu√ß√£o:**
```bash
# Local
uv run python manage.py runserver

# Docker
docker compose up -d
docker compose logs -f web
```

#### 2. "File format not supported"

**Problema:** Formato de arquivo inv√°lido.

**Solu√ß√£o:**
```bash
# Verificar formatos suportados
curl http://localhost:8000/api/formats

# Converter arquivo
ffmpeg -i input.xyz -c:a libmp3lame output.mp3
```

#### 3. "File too large"

**Problema:** Arquivo maior que o limite (padr√£o 500MB).

**Solu√ß√£o:**
```bash
# Aumentar limite no .env
MAX_AUDIO_SIZE_MB=1000

# Ou comprimir arquivo
ffmpeg -i input.mp4 -c:v libx264 -crf 28 output.mp4
```

#### 4. GPU n√£o detectada

**Problema:** GPU n√£o dispon√≠vel no container.

**Solu√ß√£o:**
```bash
# Verificar drivers NVIDIA
nvidia-smi

# Verificar runtime Docker
docker info | grep -i runtime

# Reinstalar NVIDIA Container Toolkit (veja se√ß√£o GPU)
```

#### 5. Transcri√ß√£o muito lenta

**Problema:** Modelo muito grande para CPU.

**Solu√ß√£o:**
```env
# Usar modelo menor no .env
WHISPER_MODEL=small
```

#### 6. "Out of memory"

**Problema:** Mem√≥ria insuficiente para modelo.

**Solu√ß√£o:**
```env
# Usar modelo menor
WHISPER_MODEL=base

# Ou aumentar swap/RAM
```

#### 7. V√≠deo sem √°udio

**Problema:** V√≠deo n√£o possui faixa de √°udio.

**Solu√ß√£o:**
```bash
# Verificar se v√≠deo tem √°udio
ffprobe -v error -select_streams a video.mp4

# Adicionar √°udio ao v√≠deo
ffmpeg -i video.mp4 -i audio.mp3 -c copy output.mp4
```

### Logs de Debug

```bash
# Ver logs do Django (local)
tail -f /tmp/daredevil/django.log

# Ver logs do Docker
docker compose logs -f web

# Ver logs de erro
docker compose logs web | grep -i error

# Modo debug (no .env)
DEBUG=1
LOG_LEVEL=DEBUG
```

## üèóÔ∏è Estrutura do Projeto

```
daredevil/
‚îú‚îÄ‚îÄ config/                      # Configura√ß√µes Django
‚îÇ   ‚îú‚îÄ‚îÄ settings.py             # Configura√ß√µes principais
‚îÇ   ‚îú‚îÄ‚îÄ urls.py                 # URLs do projeto
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py                 # WSGI para produ√ß√£o
‚îú‚îÄ‚îÄ transcription/              # App de transcri√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ api.py                  # Endpoints da API
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py              # Modelos Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ services.py             # L√≥gica de transcri√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ audio_processor.py      # Processamento de √°udio
‚îÇ   ‚îú‚îÄ‚îÄ video_processor.py      # Processamento de v√≠deo
‚îÇ   ‚îî‚îÄ‚îÄ portuguese_processor.py # P√≥s-processamento PT-BR
‚îú‚îÄ‚îÄ .env.example                # Exemplo de vari√°veis
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ copilot-instructions.md # Instru√ß√µes para Copilot
‚îú‚îÄ‚îÄ Dockerfile                  # Imagem Docker
‚îú‚îÄ‚îÄ docker-compose.yml          # Orquestra√ß√£o Docker
‚îú‚îÄ‚îÄ docker-entrypoint.sh        # Script de inicializa√ß√£o
‚îú‚îÄ‚îÄ manage.py                   # Django management
‚îú‚îÄ‚îÄ pyproject.toml              # Depend√™ncias (UV)
‚îú‚îÄ‚îÄ uv.lock                     # Lock de depend√™ncias
‚îú‚îÄ‚îÄ README.md                   # Este arquivo
‚îú‚îÄ‚îÄ test_*.py                   # Scripts de teste
‚îî‚îÄ‚îÄ examples.py                 # Exemplos de uso
```

## üîß Desenvolvimento

### Adicionar Nova Depend√™ncia

```bash
uv add nome-do-pacote
```

### Executar Comandos Django

```bash
# Sempre use 'uv run' antes de comandos Python
uv run python manage.py <comando>

# Exemplos:
uv run python manage.py makemigrations
uv run python manage.py migrate
uv run python manage.py createsuperuser
uv run python manage.py shell
```

### Comandos uv √öteis

```bash
# Sincronizar ambiente
uv sync

# Adicionar depend√™ncia
uv add package-name

# Remover depend√™ncia
uv remove package-name

# Atualizar depend√™ncias
uv sync --upgrade

# Executar script
uv run python script.py

# Shell Python
uv run python

# Ver vers√£o
uv --version
```

### Docker e UV

**CR√çTICO:** No ambiente Docker, sempre use `uv run` antes de comandos Python:

```bash
# ‚úÖ CORRETO
docker exec daredevil_web uv run python manage.py migrate

# ‚ùå ERRADO (n√£o encontrar√° os pacotes)
docker exec daredevil_web python manage.py migrate
```

## üìù Licen√ßa

MIT

## ü§ù Contribuindo

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou pull requests.

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìß Contato

Para d√∫vidas ou sugest√µes, abra uma issue no GitHub.

## üôè Agradecimentos

- [OpenAI Whisper](https://github.com/openai/whisper) - Modelo de transcri√ß√£o
- [Django Ninja](https://django-ninja.rest-framework.com/) - Framework de API
- [UV](https://github.com/astral-sh/uv) - Gerenciador de pacotes
- [FFmpeg](https://ffmpeg.org/) - Processamento de m√≠dia

---

**Desenvolvido com ‚ù§Ô∏è para a comunidade brasileira**

**Nota:** O modelo Whisper ser√° baixado automaticamente na primeira execu√ß√£o (~1-3GB dependendo do modelo escolhido).
