# IntegraÃ§Ã£o LLM com Qwen3:30b

## ğŸ“‹ VisÃ£o Geral

Este documento descreve a integraÃ§Ã£o do modelo LLM Qwen3:30b (via Ollama) para pÃ³s-processamento avanÃ§ado de transcriÃ§Ãµes de Ã¡udio. O LLM oferece correÃ§Ãµes mais precisas e inteligentes comparado ao processamento tradicional baseado em regras.

## ğŸ¯ Vantagens do PÃ³s-Processamento com LLM

### ComparaÃ§Ã£o: LLM vs. Tradicional

| Recurso | Processamento Tradicional | Processamento LLM (Qwen3:30b) |
|---------|---------------------------|--------------------------------|
| **CorreÃ§Ã£o Gramatical** | Baseada em regras (LanguageTool) | CompreensÃ£o contextual profunda |
| **IdentificaÃ§Ã£o de Interlocutores** | HeurÃ­sticas (pausas, perguntas) | AnÃ¡lise semÃ¢ntica do contexto |
| **RemoÃ§Ã£o de HesitaÃ§Ãµes** | Regex patterns | CompreensÃ£o do fluxo natural da fala |
| **CorreÃ§Ã£o de GÃ­rias** | Limitada | Excelente |
| **Qualidade Geral** | Boa | Excepcional |
| **Velocidade** | Muito rÃ¡pida (~1s) | Moderada (~5-15s dependendo do tamanho) |
| **Requisitos** | ConexÃ£o internet (LanguageTool) | Ollama rodando localmente |

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

1. **Instalar Ollama**
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Ou baixe em: https://ollama.ai/download
   ```

2. **Baixar o modelo Qwen3:30b**
   ```bash
   ollama pull qwen3:30b
   ```
   
   > âš ï¸ **Nota:** O modelo tem aproximadamente 17GB. Certifique-se de ter espaÃ§o em disco suficiente.

3. **Iniciar o servidor Ollama**
   ```bash
   ollama serve
   ```
   
   O servidor ficarÃ¡ disponÃ­vel em `http://localhost:11434`

### Requisitos de Hardware

- **RAM:** MÃ­nimo 16GB recomendado (32GB ideal para o modelo 30b)
- **Disco:** ~20GB de espaÃ§o livre
- **CPU/GPU:** GPU com CUDA Ã© recomendada mas nÃ£o obrigatÃ³ria

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Adicione ao seu arquivo `.env`:

```bash
# Habilitar pÃ³s-processamento LLM por padrÃ£o
USE_LLM_POST_PROCESSING=true

# Modelo LLM a ser usado
LLM_MODEL=qwen3:30b

# URL do servidor Ollama
OLLAMA_URL=http://localhost:11434/api/generate
```

### ConfiguraÃ§Ã£o Alternativa

Se preferir nÃ£o usar LLM por padrÃ£o:

```bash
# Manter processamento tradicional como padrÃ£o
USE_LLM_POST_PROCESSING=false
```

E habilitar LLM por requisiÃ§Ã£o via API (parÃ¢metro `use_llm=true`).

## ğŸ“– Uso

### Via API

#### Endpoint: POST /api/transcribe

**Com LLM habilitado:**

```bash
curl -X POST "http://localhost:8000/api/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=pt" \
  -F "use_llm=true" \
  -F "identify_speakers=true" \
  -F "correct_grammar=true"
```

**Resposta exemplo:**

```json
{
  "success": true,
  "transcription": {
    "text": "Pessoa 1: OlÃ¡, tudo bem?\nPessoa 2: Sim, estou bem. E vocÃª?\nPessoa 1: TambÃ©m estou bem, obrigado.",
    "segments": [
      {
        "start": 0.0,
        "end": 2.0,
        "text": "OlÃ¡, tudo bem?",
        "original_text": "ola tudo bem",
        "speaker_id": "Pessoa 1",
        "confidence": 0.95
      },
      {
        "start": 2.5,
        "end": 4.5,
        "text": "Sim, estou bem. E vocÃª?",
        "original_text": "sim to bem e vc",
        "speaker_id": "Pessoa 2",
        "confidence": 0.92
      }
    ],
    "formatted_conversation": "Pessoa 1: OlÃ¡, tudo bem?\nPessoa 2: Sim, estou bem. E vocÃª?\nPessoa 1: TambÃ©m estou bem, obrigado.",
    "post_processed": true
  }
}
```

### Via CÃ³digo Python

```python
from transcription.services import TranscriptionService

# Processar Ã¡udio com LLM
result = TranscriptionService.process_audio_file(
    file_path="audio.mp3",
    language="pt",
    use_llm=True,  # Habilitar LLM
    correct_grammar=True,
    identify_speakers=True,
    clean_hesitations=True
)

print(result.transcription.text)
print(result.transcription.formatted_conversation)
```

### Script de DemonstraÃ§Ã£o

Execute o script de demonstraÃ§Ã£o incluÃ­do:

```bash
uv run python demo_llm_post_processing.py
```

Este script mostra:
- VerificaÃ§Ã£o da conexÃ£o com Ollama
- CorreÃ§Ã£o gramatical avanÃ§ada
- IdentificaÃ§Ã£o inteligente de interlocutores
- RemoÃ§Ã£o de hesitaÃ§Ãµes
- Exemplos prÃ¡ticos de uso

## ğŸ§ª Testes

### Executar Testes

```bash
# Todos os testes de pÃ³s-processamento
uv run python -m unittest transcription.test_post_processing -v

# Apenas testes LLM
uv run python -m unittest transcription.test_llm_post_processing -v
```

### Cobertura de Testes

Os testes LLM incluem:
- âœ… InicializaÃ§Ã£o do serviÃ§o
- âœ… ConstruÃ§Ã£o de prompts
- âœ… Processamento bem-sucedido
- âœ… IdentificaÃ§Ã£o de interlocutores
- âœ… Tratamento de erros (timeout, conexÃ£o)
- âœ… Parsing de diferentes formatos de marcadores
- âœ… Pipeline completo de integraÃ§Ã£o

## ğŸ”§ SoluÃ§Ã£o de Problemas

### Erro: "Connection error" ou "Cannot connect to Ollama"

**Causa:** Ollama nÃ£o estÃ¡ rodando ou nÃ£o estÃ¡ acessÃ­vel.

**SoluÃ§Ã£o:**
```bash
# Verificar se Ollama estÃ¡ rodando
curl http://localhost:11434/api/tags

# Se nÃ£o estiver, inicie o servidor
ollama serve
```

### Erro: "Model not found: qwen3:30b"

**Causa:** O modelo nÃ£o foi baixado.

**SoluÃ§Ã£o:**
```bash
ollama pull qwen3:30b
```

### Processamento Muito Lento

**Causa:** Hardware limitado ou modelo muito grande.

**SoluÃ§Ãµes:**
1. Use uma GPU com CUDA se disponÃ­vel
2. Considere usar um modelo menor (mais rÃ¡pido):
   ```bash
   ollama pull qwen2.5:7b
   ```
   E configure:
   ```bash
   LLM_MODEL=qwen2.5:7b
   ```

### Fallback AutomÃ¡tico

Se o LLM falhar (timeout, erro de conexÃ£o, etc), o sistema automaticamente:
1. Registra o erro no log
2. Retorna a transcriÃ§Ã£o original sem processamento
3. Continua funcionando normalmente

## ğŸ“Š Performance

### Benchmarks TÃ­picos

| DuraÃ§Ã£o do Ãudio | Tempo de TranscriÃ§Ã£o (Whisper) | Tempo LLM (Qwen3:30b) | Total |
|------------------|--------------------------------|----------------------|-------|
| 10 segundos | ~2s | ~5s | ~7s |
| 30 segundos | ~5s | ~8s | ~13s |
| 1 minuto | ~8s | ~12s | ~20s |
| 5 minutos | ~30s | ~25s | ~55s |

> **Nota:** Tempos variam baseado em hardware. GPU acelera significativamente.

## ğŸ¯ Casos de Uso Recomendados

### Quando Usar LLM:

âœ… **Conversas importantes** - reuniÃµes, entrevistas
âœ… **MÃºltiplos interlocutores** - identificaÃ§Ã£o mais precisa
âœ… **Ãudio com muitos erros** - correÃ§Ãµes mais inteligentes
âœ… **GÃ­rias e linguagem informal** - melhor compreensÃ£o
âœ… **Qualidade final importa mais que velocidade**

### Quando Usar Processamento Tradicional:

âœ… **Processamento em tempo real** - muito mais rÃ¡pido
âœ… **Grande volume de Ã¡udios** - menor uso de recursos
âœ… **Hardware limitado** - nÃ£o requer GPU
âœ… **Ãudio jÃ¡ de boa qualidade** - diferenÃ§a mÃ­nima

## ğŸ” SeguranÃ§a e Privacidade

### Dados Locais

- âœ… Todo processamento ocorre localmente via Ollama
- âœ… Nenhum dado Ã© enviado para servidores externos
- âœ… Sem necessidade de API keys ou autenticaÃ§Ã£o externa
- âœ… Totalmente offline (apÃ³s download do modelo)

## ğŸš€ PrÃ³ximos Passos

### Melhorias Planejadas

1. **Suporte a mÃºltiplos modelos**
   - Permitir escolha do modelo via API
   - Suporte a modelos especializados por domÃ­nio

2. **Cache inteligente**
   - Cachear correÃ§Ãµes frequentes
   - Reduzir tempo de processamento para Ã¡udios similares

3. **Processamento em lote otimizado**
   - Processar mÃºltiplos Ã¡udios em paralelo
   - Melhor uso de GPU

4. **Fine-tuning**
   - Treinar modelo especÃ­fico para portuguÃªs brasileiro
   - Melhorar identificaÃ§Ã£o de sotaques regionais

## ğŸ“š Recursos Adicionais

- [DocumentaÃ§Ã£o Ollama](https://github.com/ollama/ollama)
- [Modelo Qwen](https://github.com/QwenLM/Qwen)
- [API Daredevil](README.md)

## ğŸ’¡ Exemplos de TransformaÃ§Ãµes

### Antes (TranscriÃ§Ã£o Bruta):
```
ola tudo bem com vc ah sim to bem e vc tambem to legal vamos comecar entao
```

### Depois (LLM - Qwen3:30b):
```
Pessoa 1: OlÃ¡, tudo bem com vocÃª?
Pessoa 2: Sim, estou bem. E vocÃª?
Pessoa 1: TambÃ©m estou legal. Vamos comeÃ§ar entÃ£o.
```

## ğŸ¤ Contribuindo

Encontrou um bug ou tem uma sugestÃ£o? Abra uma issue no GitHub!

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a mesma licenÃ§a do projeto Daredevil principal.

---

**Desenvolvido com â¤ï¸ para melhorar a transcriÃ§Ã£o de Ã¡udio em portuguÃªs usando IA**
